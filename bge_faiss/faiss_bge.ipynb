{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create vector database to store summarised text embeddings <br>\n",
    "2. retrieve the embeddings of the documents using semantic search and key word search <br>\n",
    "3. return top k results through re-ranking <br>\n",
    "Note: vectors stored in the database should have metadata so original non-summarised text can be retrieved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:36:16.547738Z",
     "iopub.status.busy": "2025-03-22T13:36:16.547294Z",
     "iopub.status.idle": "2025-03-22T13:36:21.235071Z",
     "shell.execute_reply": "2025-03-22T13:36:21.233737Z",
     "shell.execute_reply.started": "2025-03-22T13:36:16.547703Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import faiss\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file with data\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    loaded_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:36:21.236978Z",
     "iopub.status.busy": "2025-03-22T13:36:21.236615Z",
     "iopub.status.idle": "2025-03-22T13:36:26.665521Z",
     "shell.execute_reply": "2025-03-22T13:36:26.664565Z",
     "shell.execute_reply.started": "2025-03-22T13:36:21.236943Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdbf8495ecb41578a04a645a1d7850c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded vectors shape: (350,)\n"
     ]
    }
   ],
   "source": [
    "encoded_vectors = [model.encode(summary) for original, summary in loaded_data]\n",
    "encoded_vectors = np.array(encoded_vectors)\n",
    "\n",
    "print(\"Encoded vectors shape:\", encoded_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense embeddings shape: (350, 1024)\n",
      "Sample dense embedding: [-0.02495443  0.06316107 -0.0138085  ...  0.02421267 -0.00490005\n",
      " -0.0205813 ]\n",
      "Sample embedding shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Extract dense embeddings from each encoded vector\n",
    "dense_embeddings = np.array([vec['dense_vecs'] for vec in encoded_vectors])\n",
    "\n",
    "# Check the shape of dense embeddings\n",
    "print(f\"Dense embeddings shape: {dense_embeddings.shape}\")\n",
    "\n",
    "# Print sample embedding\n",
    "sample_index = 0\n",
    "print(f\"Sample dense embedding: {dense_embeddings[sample_index]}\")\n",
    "print(f\"Sample embedding shape: {dense_embeddings[sample_index].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 350 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Normalize dense embeddings\n",
    "norms = np.linalg.norm(dense_embeddings, axis=1, keepdims=True)\n",
    "dense_embeddings_normalized = dense_embeddings / norms\n",
    "\n",
    "dense_embeddings_normalized = dense_embeddings_normalized.astype('float32')\n",
    "\n",
    "# create FAISS index\n",
    "index = faiss.IndexFlatIP(dense_embeddings_normalized.shape[1])\n",
    "index.add(dense_embeddings_normalized)\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top documents:\n",
      "1. Original: # Text and data highlights  \n",
      "\n",
      "[Image Description: The acquisition of new solar energy fields reduces carbon emissions.]  \n",
      "\n",
      "Relationship between loyalty to the brand and aftersales satisfaction\n",
      "   Summarized: 'This section discusses the relationship between customer loyalty and aftersales satisfaction, but does not provide direct metrics or indicators related to the target list such as resource use, political spending, policy adherence, turnover rate, wastewater, total production waste, regulatory violations, water recycling, manufacturing waste, ethical violations, advertising violations, professional training, social responsibility, occupational injuries, water discharge, ethical standards, anti-corruption compliance, water consumption, misconduct reports, incident rate, waste output, financial sanctions, production energy, climate impact, inclusion, training hours, consumption rate, waste recycling, social initiatives, minority representation, emissions intensity, training cost, total energy, human capital investment, governance actions, recycling rate, electric vehicles, production byproduct, energy efficiency, production residues, regulatory incidents, corruption incidents, competition issues, monetary penalties, workforce demographics, clean mobility, compliance breaches, total waste, waste quantity, market fairness, board initiatives, diversity, anti-corruption, employee development, anti-competitive incidents, safety incidents, staff retention, recycled water percentage, carbon footprint, eco-friendly actions, landfill diversion, GHG rate, waste diversion, sustainable waste management, corporate governance, carbon efficiency, occupational hazards, total emissions, employee departure, corporate responsibility, total water, ZEV sales, average education time, greenhouse gas, training expenditure, campaign finance, reuse rate, sustainability, injury rate, marketing compliance, energy consumption, regulatory fines, incident count, workplace accidents, zero emission vehicles, circular economy, corporate citizenship, environmental initiatives, regulatory compliance, sustainability goals, industrial usage, energy intensity, effluent volume, political contributions, workplace safety, workforce stability, donation total, per vehicle, waste volume, fines, water utilization, community engagement, management practices, or Total Energy Consumed in Production, Energy Consumption per Vehicle Produced, Total Water Usage, Wastewater Volume Generated, Percentage of Water Recycled, Total GHG Emissions, GHG Intensity per Vehicle Produced, Total Manufacturing Waste Generated, Percentage of Waste Recycled or Diverted from Landfill, Number of Zero Emission Vehicles (ZEV) Sold, Amount of Total Waste from Manufacturing, Percentage of Total Waste Recycled, Environmental Related Goals and Actions Taken, Employee Turnover Rate, Number of Workplace Accidents, Injury Rates, Average Training Hours per Employee, Training Investment per Employee, Percentage of Minority Employees, Social Related Goals and Actions Taken, Number of Corruption Incidents Reported, Compliance Rate with Anti-Corruption Policies, Number of Anti-Competitive Behavior Incidents, Monetary Value of Fines Imposed, Total Value of Political Contributions Made, Number of Marketing Compliance Incidents or Violations Reported, Governance Related Goals and Actions Taken.'\n",
      "   Similarity Score: 0.3196658492088318\n",
      "2. Original: # Link icon legend  \n",
      "\n",
      "CSR macro risk symbol and issue number/title from materiality matrix  \n",
      "\n",
      "[Image Description: The image shows an circle with the number one inside, indicating the first ESG assessment.]  \n",
      "\n",
      "Related SDGs  \n",
      "\n",
      "[Image Description: A picture of food alongside a panel with the text \"Promoting Prosperity\".]  \n",
      "\n",
      "External documents   \n",
      "$<2.6>$ Internal section   \n",
      "reference   \n",
      "Press release   \n",
      "$\\odot$ Video   \n",
      "Website\n",
      "   Summarized: 'This section appears to be an image legend with no ESG-related content, so there is no summary.'\n",
      "   Similarity Score: 0.3196658492088318\n",
      "3. Original: # REFERENCE FOR READING THE CSR REPORT  \n",
      "\n",
      "The Report contains 8 chapters, 6 of which are dedicated to the 6 CSR macro risk (pillars), each one with an associated color  \n",
      "\n",
      "Chapter 1 is the Integrated Report dedicated to Stellantis CSR business overview ■\t Chapters 2 through 7 present the 22 Stellantis CSR issues (challenges) according to the following sections: 1. Context and Stellantis position 5. Policies to execute the strategy 2. Forward-looking vision and targets 6. Organization and resources 3. Identification and management of risks 7. Main initiatives, achievements and results and opportunities 8. Detailed key performance indicators 4. Governance and decision bodies to lead actions  \n",
      "\n",
      "Chapter 2 is the Climate Report and presents common sections 1. to 4. for its three CSR issues.   \n",
      "Sections 5. to 8. are presented for each CSR issue.  \n",
      "\n",
      "Chapter 6 presents common sections 4. to 6. for industrial waste (6.4), pollution (6.5), water (6.6) and biodiversity (6.7). The remaining sections are presented for each CSR issue.  \n",
      "\n",
      "Chapter 7 presents common sections for Responsible Purchasing (7.1) and Human Rights (7.2) including the global approach (with sections 3. to 5.), a focus on Human Rights in the supply chain (with related sections 1. to 7.), and a link to Human Rights in own workforce.  \n",
      "\n",
      "■\t Chapter 8 covers the methodology and auditor’s report.  \n",
      "\n",
      "[Image Description: Untitled]  \n",
      "\n",
      "Each Forward-looking vision and targets section includes a commitment scoreboard composed of:  \n",
      "\n",
      "[Image Description: Europe PC&LT dist tutorials.]  \n",
      "\n",
      "Throughout the Report there are boxes that indicate focus areas:  \n",
      "\n",
      "[Image Description: The word strategy in blue.]  \n",
      "\n",
      "Additional information in note area  \n",
      "\n",
      "[Image Description: Purple shades on the sports team.]\n",
      "   Summarized: 'This section provides an overview of the CSR report structure, including 8 chapters covering various CSR macro risks and issues, such as climate, industrial waste, pollution, water, biodiversity, responsible purchasing, and human rights.', ''\n",
      "   Similarity Score: 0.3196658492088318\n",
      "4. Original: # Powered By Our Diversity, We Lead The Way The World Moves  \n",
      "\n",
      "# 2023 CORPORATE SOCIAL RESPONSIBILITY REPORT  \n",
      "\n",
      "REFERENCE FOR READING THE CSR REPOR 3  \n",
      "\n",
      "ROM THE CHAIRMAN & THE CE 5  \n",
      "\n",
      "1\t BUSINESS MODEL AND GOVERNANCE: CREATING SHARED AND LASTING VALUE - INTEGRATED REPORT 8  \n",
      "\n",
      "A TANGIBLE IMPACT ON CLIMATE CHANGE - CLIMATE REPORT 30  \n",
      "\n",
      "3\tDRIVING THE COMPANY TRANSFORMATION THROUGH THE DEVELOPMENT OF HUMAN CAPITAL 79  \n",
      "\n",
      "4\tMEETING CHANGING CUSTOMER EXPECTATIONS ON MOBILITY  \n",
      "\n",
      "144  \n",
      "\n",
      "5\tPREVENTING ETHICS VIOLATIONS BY PROMOTING OUR ETHICAL CULTURE 174  \n",
      "\n",
      "6\tPROMOTING PROTECTION AND IMPLEMENTING RESPONSIBLE USE OF NATURAL RESOURCES 201  \n",
      "\n",
      "7\tENSURING PROTECTION OF HUMAN RIGHTS AND SUPPORTING A BALANCED ECONOMIC DEVELOPMENT OF TERRITORIES  \n",
      "\n",
      "8 APPENDI 288\n",
      "   Summarized: 'This section lacks explicit ESG-related content and does not provide sufficient information for a meaningful summary.'\n",
      "   Similarity Score: 0.3196658492088318\n",
      "5. Original: # CORPORATE SOCIAL RESPONSIBILITY REPORT  \n",
      "\n",
      "[Image Description: A picture of the number 02 with blue lines.]\n",
      "   Summarized: 'This section reports on corporate social responsibility, including employee turnover rate, number of workplace accidents, injury rates, average training hours per employee, training investment per employee, percentage of minority employees, social related goals and actions taken, number of corruption incidents reported, compliance rate with anti-corruption policies, number of anti-competitive behavior incidents, monetary value of fines imposed, total value of political contributions made, number of marketing compliance incidents or violations reported, and governance related goals and actions taken.'\n",
      "   Similarity Score: 0.3196658492088318\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"What is corporate social responsibility?\"\n",
    "\n",
    "# Generate dense embedding for the query\n",
    "query_embedding = model.encode([query], batch_size=1)['dense_vecs']\n",
    "query_embedding = np.array(query_embedding).astype('float32')\n",
    "\n",
    "# Normalize query embedding\n",
    "query_norm = np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "query_embedding_normalized = query_embedding / query_norm\n",
    "\n",
    "# Perform semantic search\n",
    "top_k = 5  # No. of results to retrieve\n",
    "distances, indices = index.search(query_embedding_normalized, top_k)\n",
    "\n",
    "# Retrieve the top-k documents and their metadata\n",
    "top_documents = [loaded_data[i] for i in indices[0]]\n",
    "top_original_texts = [loaded_data[i]['Original'] for i in indices[0]]\n",
    "\n",
    "print(\"Top documents:\")\n",
    "for i, doc in enumerate(top_documents):\n",
    "    print(f\"{i + 1}. Original: {doc['Original']}\")\n",
    "    print(f\"   Summarized: {doc['Summarized']}\")\n",
    "    print(f\"   Similarity Score: {distances[0][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ryant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ryant/nltk_data'\n    - 'c:\\\\Users\\\\ryant\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'c:\\\\Users\\\\ryant\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ryant\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ryant\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Tokenize the original texts\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m tokenized_corpus \u001b[38;5;241m=\u001b[39m [tokenize(doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummarized\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m loaded_data]\n",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Tokenize the original texts\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m tokenized_corpus \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSummarized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m loaded_data]\n",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize\u001b[39m(text):\n\u001b[1;32m----> 4\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ENGLISH_STOP_WORDS]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\ryant\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ryant\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\ryant\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryant\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryant\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\ryant\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ryant/nltk_data'\n    - 'c:\\\\Users\\\\ryant\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'c:\\\\Users\\\\ryant\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ryant\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ryant\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in ENGLISH_STOP_WORDS]\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the original texts\n",
    "tokenized_corpus = [tokenize(doc['Summarized']) for doc in loaded_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"corporate social responsibility\"\n",
    "tokenized_query = tokenize(query)\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "top_k = 5\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "# Retrieve the top-k documents and their metadata\n",
    "top_documents = [loaded_data[i] for i in top_indices]\n",
    "top_scores = [scores[i] for i in top_indices]\n",
    "\n",
    "print(\"Top documents (BM25):\")\n",
    "for i, doc in enumerate(top_documents):\n",
    "    print(f\"{i + 1}. Original: {doc['Original']}\")\n",
    "    print(f\"   Summarized: {doc['Summarized']}\")\n",
    "    print(f\"   BM25 Score: {top_scores[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Semantic and Keyword Search"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
