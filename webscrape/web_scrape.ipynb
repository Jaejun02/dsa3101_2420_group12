{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import shutil\n",
    "\n",
    "def webscrape_report_urls():\n",
    "    \"\"\"\n",
    "    This function scrapes the responsbilityreports.com website to find Industries that \n",
    "    falls under Automotive Industries.\n",
    "    Input: \n",
    "        None.\n",
    "    Returns:\n",
    "        download_urls -> List[String]: List of URLs to download ESG Reports.\n",
    "        download_dests -> List[String]: List of destinations to save the downloaded ESG Reports.\n",
    "    \"\"\"\n",
    "    # Step 1: Get List of All Industries (To Find Industries under Automotive Industry)\n",
    "    response = requests.get(\"https://www.responsibilityreports.com/Browse/Industry\")\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Step 2: Filter Industry Names and Links under Automotive Industry\n",
    "    industry_link = soup.find_all(\"a\", href=re.compile(r'/Companies\\?ind=i\\d'))\n",
    "    industry_link = [(i['href'], i.text.strip()) for i in industry_link]\n",
    "    link, industry = zip(*industry_link)\n",
    "    automotive_url, automotive_industry = [], []\n",
    "    for i, ind in enumerate(industry):\n",
    "        if ind in [\"Auto Manufacturers - Major\", \"Auto Parts\", \"Auto Dealerships\", \"Auto Parts Stores\", \"Auto Parts Wholesale\", \"Trucks & Other Vehicles\"]:\n",
    "            automotive_url.append(\"https://www.responsibilityreports.com\" + link[i])\n",
    "            automotive_industry.append(ind)\n",
    "\n",
    "    # Step 3: Retrieve ESG Report URLs for Companies under Automotive Industry\n",
    "    download_urls = []\n",
    "    download_dests = []\n",
    "    for i, url in enumerate(automotive_url):\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        companies = soup.find_all(\"a\", href=re.compile(r'/Company/[a-z\\\\-]+'))\n",
    "        companies = [(i['href'], i.text.strip()) for i in companies]\n",
    "        company_link, company_name = zip(*companies)\n",
    "        company_link = [\"https://www.responsibilityreports.com/\" + i for i in company_link]\n",
    "        for j, company in enumerate(company_link):\n",
    "            response = requests.get(company)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            report = soup.find('a', href=re.compile(r'/Click/\\d+'))\n",
    "            if report:\n",
    "                report_link, report_name = report['href'], report.text.strip()\n",
    "                report_link = \"https://www.responsibilityreports.com/\" + report_link\n",
    "                download_urls.append(report_link)\n",
    "                download_dests.append(f\"../rr_data/{automotive_industry[i]}/{company_name[j]}/{report_name}.pdf\")\n",
    "    return download_urls, download_dests\n",
    "\n",
    "def download_helper(url, output_path):\n",
    "    \"\"\"\n",
    "    This function downloads the PDF from the given single URL and saves it to the output path.\n",
    "    It additionally logs the Industry and Company names, and url to a text file.\n",
    "    Input:\n",
    "        url -> String: URL to download the PDF.\n",
    "        output_path -> String: Path to save the downloaded PDF.\n",
    "    Returns:\n",
    "        Success -> boolean: True if the PDF is downloaded successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(output_path, 'wb') as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "        with open(\"../rr_data/ResponsbilityReports.txt\", 'a') as file:\n",
    "            file.write(f\"Industry: {output_path.split(\"/\")[2]}, Company: {output_path.split(\"/\")[3]}\\n\")\n",
    "        with open(\"../rr_data/pdf_reports_urls.txt\", 'a') as file:\n",
    "            file.write(url+'\\n')\n",
    "        return True\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error downloading the PDF: {e}\")\n",
    "        return False\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving the PDF: {e}\")\n",
    "        return False\n",
    "\n",
    "def download_reports():\n",
    "    \"\"\"\n",
    "    This function downloads the ESG Reports from the URLs obtained from webscrape_report_urls function.\n",
    "    Input:\n",
    "        None.\n",
    "    Returns:\n",
    "        Result -> String: Summary statement of the download process.\n",
    "    \"\"\"\n",
    "    download_urls, download_dests = webscrape_report_urls()\n",
    "    results = []\n",
    "    for i, url in enumerate(download_urls):\n",
    "        results.append(download_helper(url, download_dests[i]))\n",
    "    return f\"\\rSuccessfully Downloaded {sum(results)}/{len(results)} PDFs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://www.responsibilityreports.com/Browse/Industry\")\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_link = soup.find_all(\"a\", href=re.compile(r'/Companies\\?ind=i\\d'))\n",
    "industry_link = [(i['href'], i.text.strip()) for i in industry_link]\n",
    "link, industry = zip(*industry_link)\n",
    "automotive_url, automotive_industry = [], []\n",
    "for i, ind in enumerate(industry):\n",
    "    if ind in [\"Auto Manufacturers - Major\", \"Auto Parts\", \"Auto Dealerships\", \"Auto Parts Stores\", \"Auto Parts Wholesale\", \"Trucks & Other Vehicles\"]:\n",
    "        automotive_url.append(\"https://www.responsibilityreports.com\" + link[i])\n",
    "        automotive_industry.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_urls = []\n",
    "download_dests = []\n",
    "for i, url in enumerate(automotive_url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    companies = soup.find_all(\"a\", href=re.compile(r'/Company/[a-z\\\\-]+'))\n",
    "    companies = [(i['href'], i.text.strip()) for i in companies]\n",
    "    company_link, company_name = zip(*companies)\n",
    "    company_link = [\"https://www.responsibilityreports.com/\" + i for i in company_link]\n",
    "    for j, company in enumerate(company_link):\n",
    "        response = requests.get(company)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        report = soup.find('a', href=re.compile(r'/Click/\\d+'))\n",
    "        if report:\n",
    "            report_link, report_name = report['href'], report.text.strip()\n",
    "            report_link = \"https://www.responsibilityreports.com/\" + report_link\n",
    "            download_urls.append(report_link)\n",
    "            download_dests.append(f\"../rr_data/{automotive_industry[i]}/{company_name[j]}/{report_name}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url, output_path):\n",
    "    try:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(output_path, 'wb') as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "        with open(\"../rr_data/ResponsbilityReports.txt\", 'a') as file:\n",
    "            file.write(f\"Industry: {output_path.split(\"/\")[2]}, Company: {output_path.split(\"/\")[3]}\\n\")\n",
    "        with open(\"../rr_data/pdf_reports_urls.txt\", 'a') as file:\n",
    "            file.write(url+'\\n')\n",
    "        return True\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error downloading the PDF: {e}\")\n",
    "        return False\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving the PDF: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pdf_url in enumerate(download_urls):\n",
    "    download_pdf(pdf_url, download_dests[i])\n",
    "    print(f\"\\rDownloaded {i+1}/{len(download_urls)} PDFs\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
