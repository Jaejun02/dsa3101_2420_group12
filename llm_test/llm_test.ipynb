{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:55:00.833380Z",
     "iopub.status.busy": "2025-03-23T12:55:00.833112Z",
     "iopub.status.idle": "2025-03-23T12:55:40.083198Z",
     "shell.execute_reply": "2025-03-23T12:55:40.082228Z",
     "shell.execute_reply.started": "2025-03-23T12:55:00.833352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:56:04.732301Z",
     "iopub.status.busy": "2025-03-23T12:56:04.731991Z",
     "iopub.status.idle": "2025-03-23T12:56:04.759160Z",
     "shell.execute_reply": "2025-03-23T12:56:04.758448Z",
     "shell.execute_reply.started": "2025-03-23T12:56:04.732279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_name = \"esg-report-v1/pdf_17_processed.txt\"\n",
    "with open(file_name, 'r') as file:\n",
    "    pdf_report = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing tables from html to csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:56:05.038193Z",
     "iopub.status.busy": "2025-03-23T12:56:05.037764Z",
     "iopub.status.idle": "2025-03-23T12:56:05.045402Z",
     "shell.execute_reply": "2025-03-23T12:56:05.044283Z",
     "shell.execute_reply.started": "2025-03-23T12:56:05.038160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def html_to_csv(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    def get_cell_text(cell):\n",
    "        return cell.get_text(strip=True)\n",
    "    rows = []\n",
    "    for tr in table.find_all('tr'):\n",
    "        cells = tr.find_all(['td', 'th'])\n",
    "        row = [get_cell_text(cell) for cell in cells]\n",
    "        rows.append(row)\n",
    "    op = \"\"\n",
    "    num_col = 0\n",
    "    for i, row in enumerate(rows):\n",
    "        if i == 0:\n",
    "            num_col += len(row)\n",
    "        else:\n",
    "            cur_len = len(row)\n",
    "            if cur_len > num_col:\n",
    "                remove_len = cur_len - num_col\n",
    "                rmd = 0\n",
    "                for i in range(remove_len):\n",
    "                    if rmd >= remove_len:\n",
    "                        break\n",
    "                    for i, elem in enumerate(row):\n",
    "                        if len(elem) <= 1:\n",
    "                            row = row[:i] + row[i+1:]\n",
    "                            break\n",
    "            elif cur_len < num_col:\n",
    "                row.extend([\"\" for i in range(num_col - cur_len)])\n",
    "        for i, elem in enumerate(row):\n",
    "            if elem == \"\":\n",
    "                elem = '\"\"'\n",
    "            if re.findall(r\"(\\d+,\\d+)+\", elem):\n",
    "                elem = re.sub(\",\", \"\", elem)\n",
    "            if re.findall(r\",\", elem):\n",
    "                elem = f\"[{elem}]\"\n",
    "            row[i] = elem\n",
    "        op += \", \".join(row)\n",
    "        op += \"\\n\"\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:56:07.165856Z",
     "iopub.status.busy": "2025-03-23T12:56:07.165502Z",
     "iopub.status.idle": "2025-03-23T12:56:07.724663Z",
     "shell.execute_reply": "2025-03-23T12:56:07.724009Z",
     "shell.execute_reply.started": "2025-03-23T12:56:07.165825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pdf_report_tt = \"\"\n",
    "for para in pdf_report.split(\"\\n\\n\"):\n",
    "    if re.findall(\"<html>.+</html>\", para):\n",
    "        para = html_to_csv(para)\n",
    "        para = '```{table}\\n' + para\n",
    "        para = para + '```'\n",
    "    pdf_report_tt += para\n",
    "    pdf_report_tt += \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing pdf into sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:56:07.725838Z",
     "iopub.status.busy": "2025-03-23T12:56:07.725627Z",
     "iopub.status.idle": "2025-03-23T12:56:07.800400Z",
     "shell.execute_reply": "2025-03-23T12:56:07.799535Z",
     "shell.execute_reply.started": "2025-03-23T12:56:07.725820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(?s)(#.*?\\n.*?\\S)(?=\\s*#|$)') # Matches each header sections.\n",
    "sections = pattern.findall(pdf_report_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading LLM Model for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:56:07.801997Z",
     "iopub.status.busy": "2025-03-23T12:56:07.801675Z",
     "iopub.status.idle": "2025-03-23T12:56:07.818354Z",
     "shell.execute_reply": "2025-03-23T12:56:07.817661Z",
     "shell.execute_reply.started": "2025-03-23T12:56:07.801965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:56:07.968649Z",
     "iopub.status.busy": "2025-03-23T12:56:07.968379Z",
     "iopub.status.idle": "2025-03-23T12:58:00.081736Z",
     "shell.execute_reply": "2025-03-23T12:58:00.081006Z",
     "shell.execute_reply.started": "2025-03-23T12:56:07.968627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", quantization_config=quant_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading ESG Relevant Data Stored in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:58:00.083447Z",
     "iopub.status.busy": "2025-03-23T12:58:00.083134Z",
     "iopub.status.idle": "2025-03-23T12:58:00.097824Z",
     "shell.execute_reply": "2025-03-23T12:58:00.097021Z",
     "shell.execute_reply.started": "2025-03-23T12:58:00.083417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"esg_indicators.json\", 'r') as file:\n",
    "    esg_indicators = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:58:42.803766Z",
     "iopub.status.busy": "2025-03-23T12:58:42.803450Z",
     "iopub.status.idle": "2025-03-23T12:58:42.807630Z",
     "shell.execute_reply": "2025-03-23T12:58:42.806898Z",
     "shell.execute_reply.started": "2025-03-23T12:58:42.803739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "INDICATORS = list(esg_indicators.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:58:53.049455Z",
     "iopub.status.busy": "2025-03-23T12:58:53.049159Z",
     "iopub.status.idle": "2025-03-23T12:58:53.056965Z",
     "shell.execute_reply": "2025-03-23T12:58:53.056108Z",
     "shell.execute_reply.started": "2025-03-23T12:58:53.049432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('esg_context.json', 'r') as file:\n",
    "    ESG_CONTEXT = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:59:04.831340Z",
     "iopub.status.busy": "2025-03-23T12:59:04.831047Z",
     "iopub.status.idle": "2025-03-23T12:59:04.835290Z",
     "shell.execute_reply": "2025-03-23T12:59:04.834348Z",
     "shell.execute_reply.started": "2025-03-23T12:59:04.831317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "KEYWORDS = []\n",
    "[KEYWORDS.extend(indicators['keywords']) for indicators in esg_indicators.values()]\n",
    "KEYWORDS = list(set(KEYWORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt creation and summarizing\n",
    "\n",
    "TIP: For the prompt, if the summary keeps returning strange results for list of contents, can add in condition to ignore list of contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T13:00:00.381058Z",
     "iopub.status.busy": "2025-03-23T13:00:00.380692Z",
     "iopub.status.idle": "2025-03-23T13:00:00.385303Z",
     "shell.execute_reply": "2025-03-23T13:00:00.384440Z",
     "shell.execute_reply.started": "2025-03-23T13:00:00.381028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_msg = f\"\"\"You are an expert ESG reporting analyst specializing in the automotive industry. Your task is to generate a concise, retrieval-optimized summary for each section of an ESG report. Each section is introduced by markdown headers (e.g., '# Section Name') and contains detailed ESG-related information. Your summary must capture the key insights, metrics, and trends described in the section, while naturally incorporating relevant ESG terminology from {ESG_CONTEXT} and aligning with potential future changes in {INDICATORS} and {KEYWORDS}.\n",
    "\n",
    "Note: Any table appearing in the text is enclosed by \"```{{table}}...```\" and is represented in CSV format.\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the provided section carefully and extract its unique, most salient data points and insights.\n",
    "2. Write a single, clear sentence that summarizes the section in an information-dense manner. Include numerical details, trends, or context where available.\n",
    "3. Where applicable, naturally integrate relevant ESG keywords and indicator concepts from {KEYWORDS} and {INDICATORS} without simply listing them. The summary should read as a coherent description of the sectionâ€™s content.\n",
    "4. Ensure that each summary is distinct and tailored to its specific section, avoiding generic or repetitive phrasing across sections.\n",
    "5. If the section contains no meaningful content, return an empty string.\n",
    "6. The entire input (including whitespaces such as newlines) represents one section; return one summarization statement accordingly.\n",
    "\n",
    "Output:\n",
    "Return only the section header and its corresponding summary enclosed within \"<summary>...</summary>\" tags (without any markdown formatting).\n",
    "\n",
    "Example:\n",
    "Input: \"## Energy and Emissions Reduction\\n\\nIn 2023, our facilities consumed 2.5 million MWh of energy (3% reduction YoY), with energy consumption per vehicle produced dropping to 1.8 MWh/unit. Scope 1-2 GHG emissions totaled 450,000 metric tons, while ZEV sales grew to 12% of total vehicles sold...\"\n",
    "Output: \"<summary>This section outlines a 3% YoY reduction in energy consumption (2.5M MWh total and 1.8 MWh per vehicle), alongside 450k metric tons of GHG emissions and a 12% increase in ZEV sales, reflecting key energy, emissions, and ZEV performance trends.</summary>\"\n",
    "\n",
    "Here is the input section to summarize:\\n\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T13:01:50.638896Z",
     "iopub.status.busy": "2025-03-23T13:01:50.638379Z",
     "iopub.status.idle": "2025-03-23T13:14:14.224361Z",
     "shell.execute_reply": "2025-03-23T13:14:14.223351Z",
     "shell.execute_reply.started": "2025-03-23T13:01:50.638844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "summarized = []\n",
    "for section in sections[:50]:\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    section_without_newline = re.sub(\"\\n\", \" \", section)\n",
    "    ip_msg = f'\"\"\"{section_without_newline}\"\"\"'\n",
    "    msg = base_msg + ip_msg\n",
    "    messages = [{'role': 'user', 'content': msg}]\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_attention_mask=True).to(\"cuda\")\n",
    "    outputs = model.generate(inputs, max_new_tokens=100000)\n",
    "    op = tokenizer.decode(outputs[0]).split(\"<|end_header_id|>\")[-1].strip(\"\\n\").split(\"<|eot_id|>\")[0]\n",
    "    op = re.findall(r'<summary>(.*?)</summary>', op)\n",
    "    op = ' '.join(op)\n",
    "    summarized.append(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating seperate vector db for semantic search and key word search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vmodel = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "embeddings = vmodel.encode(summarized, convert_to_numpy=True)\n",
    "faiss.normalize_L2(embeddings)\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_summaries = [word_tokenize(text.lower()) for text in summarized]\n",
    "bm25 = BM25Okapi(tokenized_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Searching, Reranking, and Output\n",
    "\n",
    "TIP: You can tweak `alpha` to control the importance of semantic search over key word search. Higher the `alpha`, more weight given to the semantic search result.\n",
    "\n",
    "`semantic_query`: The query to the used for semantic search.\n",
    "\n",
    "`keyword_query_list`: The list of keywords to be sent for key word search.\n",
    "\n",
    "`rerank_k`: The top k results that will be chosen from the 2 vector dbs before reranking.\n",
    "\n",
    "`top_k`: The ultimate top k results chosen after reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(semantic_query, keyword_query_list, rerank_k=200, top_k=50, alpha=0.7):\n",
    "    # --- Semantic search ---\n",
    "    query_embedding = vmodel.encode([semantic_query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    D, I = index.search(query_embedding, rerank_k)\n",
    "    semantic_results = {i: D[0][idx] for idx, i in enumerate(I[0])}\n",
    "    \n",
    "    # --- BM25 keyword search ---\n",
    "    bm25_scores = bm25.get_scores(keyword_query_list)\n",
    "    bm25_top_indices = np.argsort(bm25_scores)[::-1][:rerank_k]\n",
    "    bm25_results = {i: bm25_scores[i] for i in bm25_top_indices}\n",
    "    \n",
    "    # --- Combine and rerank ---\n",
    "    combined_results = {}\n",
    "    \n",
    "    for i, score in semantic_results.items():\n",
    "        combined_results[i] = alpha * score\n",
    "    \n",
    "    for i, score in bm25_results.items():\n",
    "        if i in combined_results:\n",
    "            combined_results[i] += (1 - alpha) * score\n",
    "        else:\n",
    "            combined_results[i] = (1 - alpha) * score\n",
    "    \n",
    "    ranked_indices = sorted(combined_results, key=lambda i: combined_results[i], reverse=True)[:top_k]\n",
    "    \n",
    "    final_results = [(sections[i], combined_results[i]) for i in ranked_indices]\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of data. Indicator by indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The one-shot examples are stored in a JSON file\n",
    "with open(\"oneshots.json\", 'r') as file:\n",
    "    examples = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", quantization_config=quant_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for cur_indicator in INDICATORS:\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    semantic_query = cur_indicator\n",
    "    keyword_query_list = esg_indicators[cur_indicator]['keywords']\n",
    "    results = search(semantic_query, keyword_query_list, rerank_k = 200, top_k=50, alpha=0.7)\n",
    "    text_input = \"\"\n",
    "    for text, score in results:\n",
    "        if score > 0.7:\n",
    "            text_input += text\n",
    "            text_input += \"\\n\\n\"\n",
    "    msg = f\"\"\"You are an expert ESG reporting analyst specializing in the automotive industry. Your task is to extract '{cur_indicator}' indicator given a text to extract from.\n",
    "\n",
    "        Instructions:\n",
    "        \"\"\"\n",
    "    if esg_indicators[cur_indicator]['data_type'] == \"Quantitative\":\n",
    "        msg += f\"\"\"        1. The current indicator is a quantitative indicator. You must extract relevant numeric data or a short string that includes one of the possible units specified in the \"unit\" instruction.\n",
    "        2. The allowed list of units are: {esg_indicators[cur_indicator]['unit']}. For any other units found, convert into one of the units in the list.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        msg += f\"\"\"        1. The current indicator is a qualitative indicator. You must locate the sentence(s) that best describe the indicator within the text to extract.\n",
    "        2. If the relevant information is spread across disjoint sentences, return them as a separate element of a list; otherwise, return a list with single sentence.\n",
    "        \"\"\"\n",
    "    msg += f\"\"\"        3. The following key words: {esg_indicators[cur_indicator]['keywords']} are some words that you can watch out for in extraction, but consider the overall context to ensure accurate extraction.\n",
    "    4. You must follow the additional instructions specified here: {esg_indicators[cur_indicator]['extraction_notes']}.\n",
    "    5. The final output should be enclosed with <output> and </output> tags.\n",
    "    6. If there are no data to be found, return <output>No data available for {cur_indicator}</output>.\n",
    "\n",
    "    The following are some contexts that you may refer to when understanding the text to extract from:\n",
    "    Specific to '{cur_indicator}': {esg_indicators[cur_indicator]['background']}\n",
    "    General ESG background: {ESG_CONTEXT}\n",
    "\n",
    "    Example:\n",
    "    {examples[cur_indicator]}.\n",
    "\n",
    "    Here is the text to extract '{cur_indicator}' from:\n",
    "    '''\n",
    "    {text_input}\n",
    "    ''''\n",
    "    \"\"\"\n",
    "    messages = [{'role': 'user', 'content': msg}]\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_attention_mask=True).to(\"cuda\")\n",
    "    outputs = model.generate(inputs, max_new_tokens=100000)\n",
    "    op = tokenizer.decode(outputs[0]).split(\"<|end_header_id|>\")[-1].strip(\"\\n\").split(\"<|eot_id|>\")[0]\n",
    "    op = re.findall(r'<output>(.*?)</output>', op)[0]\n",
    "    res.append(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([res], columns=INDICATORS)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6847935,
     "sourceId": 11027131,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "esg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
